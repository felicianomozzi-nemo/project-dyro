"""
Main module for the documentation assistant demo.

This script uses Streamlit to create a natural language query interface
over documents previously indexed with FAISS. It connects to a local
language model running through Ollama to generate contextual answers
based on the retrieved documentation.
"""

import pickle
import streamlit as st
from sentence_transformers import SentenceTransformer
import ollama

# Load the index and documents previously generated by ingest.py
with open("store.pkl", "rb") as f:
    documents, paths, index = pickle.load(f)

# Initialize the embedding model
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Streamlit app title
st.title("ðŸ¤– Dyro")

# Text input for user query
query = st.text_input("Enter your question about the documentation:")

if query:
    # Generate embedding for the user query
    query_vec = embedder.encode([query])
    _, I = index.search(query_vec, k=2)  # Retrieve the 2 most relevant documents

    # Build the context from retrieved documents
    CONTEXT = "\n".join([documents[i] for i in I[0]])
    refs = [paths[i] for i in I[0]]

    # Prepare the prompt for the language model
    PROMPT = f"Answer based on the following documentation:\n\n{CONTEXT}\n\nQuestion: {query}"

    # Get the model's response from Ollama
    response = ollama.chat(model="gemma:2b", messages=[{"role": "user", "content": PROMPT}])

    # Display the model's answer
    st.markdown("### ðŸ“– Answer:")
    st.write(response["message"]["content"])

    # Display the referenced documents used for context
    st.markdown("### ðŸ“‚ Referenced Documents:")
    for r in refs:
        st.write(f"- {r}")
